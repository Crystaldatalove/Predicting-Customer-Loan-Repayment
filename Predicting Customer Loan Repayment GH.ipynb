
# coding: utf-8

# # Data Preparation

# *Loading traning dataset

# In[1]:

import pandas as pd
import numpy as np 

data = pd.read_csv('LoansTrainingSet.csv')
data.head()


# In[2]:

data.info()


# *As we can see from above, we have problems with some missing values & data types

# *Let's take a look at "Current Loan Amount" Column

# In[3]:

data['Current Loan Amount'].max(), data['Current Loan Amount'].min(),data['Current Loan Amount'].mean()


# *Selecting the rows of "Current Loan Amount" is lesser than 99999999

# In[4]:

data = data[(data['Current Loan Amount'] < 99999999)]


# In[5]:

data['Current Loan Amount'].describe()


# *Now, in "Current Loan Amount" column, the mean value is 13979.69 and median value is 12058.00

# *Looking at unique values of "Years in current job", "Home Ownership", and "Purpose" columns

# In[6]:

data['Years in current job'].unique()


# In[7]:

data['Home Ownership'].unique()


# In[8]:

data['Purpose'].unique()


# # Data Problems

# 1. Missing values: 
#  credit score (has nan), Annual Income, Months since last delinquent (has nan), Bankruptcies (has nan), Tax Liens (has nan)
#     
# 2. Spelling differences & punctuation format
#  Years in current job: delete years, 10+, <1, n/a;   
#  Home Ownership: Home Mortgage == HaveMortgage;
#  Purpose: Other == other;
#  Monthly Debt: delete $ ;  
#  Maximum Open Credit: why is it Object? ;
#  Years of Credit History: decimal 1 place
# 
# 3. Duplicates rows
# 
# 4. Some of Credit scores are too high, have to devide by 10
# 
# 5. Current Loan Amount: too high 9999999 compared to their annual income
# 
# 6. Data types:
#  convert Credit Score - float64 to int64;
#  convert Monthly Debt, Maximum Open Credit - object to float64

# 1. missing values: credit score (has nan)

# In[9]:

credit_score_nan = data['Credit Score'].isnull()
data[credit_score_nan].head()


# *Creating a new dataframe as df that has "Credit Score" values

# In[10]:

df = data[data['Credit Score'].notnull()]


# In[11]:

df.info()   


# *convert Credit Score column from float to int type

# In[12]:

df['Credit Score'] = df['Credit Score'].astype(np.int64)


# In[13]:

df.info()


# *Dividing "Credit Score" by 10, for the values more than 800

# In[14]:

df['Credit Score'] = df['Credit Score'].map(lambda x: x/10 if x > 800 else x)


# In[15]:

df['Credit Score'][(df['Credit Score'] > 800)].count()


# *Now, we don't have credit score more than 800

# *Convert Monthly Debt from object to float64, removing the $

# In[16]:

df['Monthly Debt'] = df['Monthly Debt'].replace( '[\$, ]','', regex=True).astype(float) 
df['Monthly Debt'].head()


# *Converting "Maximum Open Credit" from object to float64

# In[17]:

df['Maximum Open Credit'] = df['Maximum Open Credit'].convert_objects(convert_numeric=True)
df['Maximum Open Credit'].head()


# *Home Ownership: converting "HaveMortgage" to "Home Mortgage"

# In[18]:

df['Home Ownership'] = df['Home Ownership'].map(lambda x: 'Home Mortgage' if x == 'HaveMortgage' else x)


# *Purpose: converting "other" to "Other"

# In[19]:

#Purpose: other == Other

df['Purpose'] = df['Purpose'].map(lambda x: 'Other' if x == 'other' else x)


# *Checking the unique values of "Home Ownership" and "Purpose"

# In[20]:

print "Home Ownership: ", df['Home Ownership'].unique()
print "Purpose: ", df['Purpose'].unique()


# *We have missing values in "Month since last delinquent" column. Let's take a look at the relationship between NA values in "Month since last delinquent" column and other "problem columns" such as "Purpose", "Number of Credit Problems", "Bankruptcies", and "Tax Liens"

# In[21]:

msld_nan = df['Months since last delinquent'].isnull()

print df[msld_nan]['Purpose'].value_counts()

print df[msld_nan]['Number of Credit Problems'].value_counts()

print df[msld_nan]['Bankruptcies'].value_counts()

print df[msld_nan]['Tax Liens'].value_counts()


# *Most of NA values in "Month since last delinquent" column have: purpose of Debt Consolidation; 0 Number of Credit Problems; 0 Bankruptcies; 0 Tax Liens

# *Now, taking a look at correlations

# In[22]:

df.corr()


# *Let's take a look at "Months since last delinquent" column

# In[23]:

df['Months since last delinquent'].max()


# *Let's fill in NA in 'Months since last delinquent' with 200 which is even more than max number of credit problems (176), because we noticed above that most NA in 'Months since last delinquent' has "least" problems

# In[24]:

df["Months since last delinquent"].fillna(value=200, inplace=True)


# In[25]:

df['Months since last delinquent'].describe()


# In[26]:

df.info()


# *Tax Liens is highly correlated with Number of credit problems = 0.59, so let's only include none missing values in Tax Liens for our new dataframe df

# In[27]:

df = df[df['Tax Liens'].notnull()]


# In[28]:

df.info()


# *Let's only include none missing values in Bankruptcies for our new dataframe df

# In[29]:

df = df[df['Bankruptcies'].notnull()] 


# In[30]:

df.info()


# *Taking a look at "Maximum Open Credit" column

# In[31]:

moc_nan = df['Maximum Open Credit'].isnull()
moc_nan.value_counts()


# *Selecting none missing values in Maximum Open Credit for our new dataframe df

# In[32]:

df = df[df['Maximum Open Credit'].notnull()]


# In[33]:

df.info()


# *Now, we don't have any missing values

# *Taking look at "Loan ID" and "Customer ID" columns

# In[34]:

print "Total unique Loan IDs: ", df['Loan ID'].nunique()
print "Total unique Customer IDs: ", df['Customer ID'].nunique()
print "Total entries: ", len(df)


# In[35]:

df['Loan ID'].value_counts()


# *Let's drop the duplicate Loan ID values & keep first row, and set new dataframe df

# In[36]:

df = df.drop_duplicates(['Loan ID'], keep ='first') 


# In[37]:

print "Total unique Loan IDs: ", df['Loan ID'].nunique()
print "Total unique Customer IDs: ", df['Customer ID'].nunique()
print "Total entries: ", len(df)


# In[38]:

df.info()


# *Double check that we don't have duplicated rows

# In[39]:

print "Total unique Customer IDs: ", df['Customer ID'].nunique()
print "Total entries: ", len(df)


# *Let's take a look at "Years in current job" column - removing the strings by deleting "years and +"; converting it to int; converting n/a and <1 to 0

# In[40]:

df['Years in current job'] = df['Years in current job'].map(lambda x: '0' if x == 'n/a' else x) 
df['Years in current job'] = df['Years in current job'].map(lambda x: '0' if x == '< 1 year' else x)
df['Years in current job'] = df['Years in current job'].replace( '[\+ years]','', regex=True).astype(np.int64) 


# In[41]:

df['Years in current job'].unique()


# *Now, we we have integers only for "Years in current job" column

# *Converting "Loan Status" to binary output for easy modeling : Fully paid = 1

# In[42]:

df['Loan Status'] = df['Loan Status'].map(lambda x: 1 if x == 'Fully Paid' else 0) 


# In[43]:

df['Loan Status'].unique()


# *Converting "Term" to binary output for easy modeling: Long Term  = 1

# In[44]:

df['Term'] = df['Term'].map(lambda x: 1 if x == 'Long Term' else 0)


# In[45]:

df['Term'].unique()


# In[46]:

df.info()


# *Feature engineering 1: Credit Utilization Rate = sum of outstanding balance / credit card's limit; 
# *In our case, Credit Utilization Rate = Current Credit Balance / Maximum Open Credit

# In[47]:

df['Credit Utilization'] = df['Current Credit Balance'].div(df['Maximum Open Credit'] + 1, axis='index')


# In[48]:

df['Credit Utilization'].describe()


# *Feature engineering 2: Payment Rate = (Monthly Debt ** 12) / Annual Income = Annual Payment / Annual Income ;     Add 1 to this value

# In[49]:

df['Annual Payment'] = df['Monthly Debt']*12


# In[50]:

df['Payment Rate'] = df['Annual Payment'].div(df['Annual Income'] + 1, axis='index')


# In[51]:

df['Payment Rate'].describe()


# *Renaming all the columns to make it simple

# In[52]:

df = df.rename(columns={'Loan Status': 'y', 
                        'Current Loan Amount': 'a', 
                        'Term': 'b', 'Credit Score': 'c', 
                        'Years in current job': 'd', 
                        'Home Ownership': 'e', 
                        'Annual Income': 'f', 
                        'Purpose': 'g', 'Monthly Debt': 'h', 
                        'Years of Credit History': 'i', 
                        'Number of Open Accounts': 'j', 
                        'Number of Credit Problems': 'k', 
                        'Current Credit Balance': 'l', 
                        'Maximum Open Credit': 'm', 
                        'Bankruptcies': 'n', 'Tax Liens': 'o',
                        'Credit Utilization': 'p', 'Payment Rate': 'q'})
df


# # Data Visualization

# In[53]:

import matplotlib.pyplot as plt
get_ipython().magic('matplotlib inline')
import seaborn as sns


# *Plotting "Current Loan Amount" for histogram and boxplot

# In[54]:

fig = plt.figure()
ax1 = fig.add_subplot(121)
ax2 = fig.add_subplot(122)

ax1.hist(df.a)
ax2.boxplot(df.a)


# *Normalizing it by applying log into "Current Loan Amount"

# In[55]:

plt.hist(np.log(df.a))


# *Now we have the histogram that is skewed to the left

# *Now we want to plot x as "Credit Utilization" & y as "Loan Status"

# In[56]:

x = df['p']
y = df['y']

# make the plot
fig = plt.figure()
ax = fig.add_subplot(111)  
ax.scatter(x, y)
ax.set_ylabel('Loan Status', fontsize=20)  
ax.set_xlabel('Credit Utilization', fontsize=20)


# *Note that Loan Status = 1 is Fully Paid and it's more dense on the left; We can conclude that customers who have Loan Status of Fully Paid tend to have lower credit utilization rate

# *Now we want to create the dummy variables for categorical values using dmatrices

# In[57]:

from patsy import dmatrices
y, X = dmatrices('y ~ a + b + c + d + C(e) + f + C(g) + h + i + j + k + l + m + n + o + p + q', df, return_type = 'dataframe')


# In[58]:

X.head()


# In[59]:

y.head()


# In[60]:

X.shape, y.shape


# # Modeling

# In[61]:

from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0)


# 1. Logistic Regression

# In[62]:

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()
logreg.fit(X_train, y_train)
logreg.score(X_test, y_test)


# In[63]:

y_pred_loanstatus = logreg.predict(X_test)


# In[64]:

from sklearn import metrics
print metrics.accuracy_score(y_pred_loanstatus, y_test)


# In[65]:

report = metrics.classification_report(y_pred_loanstatus, y_test)
print report


# *Logistic Regression: accuracy score = 0.73; f1 score = 0.84

# *Drop Intercept column to fit different models

# In[66]:

X = X.drop(['Intercept'], axis = 1)


# In[67]:

from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 0)


#  2.. Random Forest Classifier

# *Let's take a look at shapes of X, y, X_train, y_train, X_test, y_test

# In[68]:

X.shape, y.shape


# In[69]:

X_train.shape, y_train.shape


# In[70]:

X_test.shape, y_test.shape


# *1st method: GridSearchCV

# *Converting y_train to array

# In[71]:

y_train = np.ravel(y_train)


# In[72]:

y_train.shape 


# In[73]:

from sklearn.grid_search import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()

param_grid = {"n_estimators": [100, 200, 300],
              "max_features": [3, 5],
              "max_depth": [10, 20],
              "min_samples_split": [2, 4]}

grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, n_jobs=-1, cv=2)
grid_search.fit(X_train, y_train)
print grid_search.best_params_
print grid_search.best_score_
print grid_search.best_estimator_


# *Now, we can put "best" parameters into random forest classifier;   
# *Fitting in training, scoring in testing

# In[74]:

rfc = RandomForestClassifier(max_features= 5, min_samples_split= 2, n_estimators= 100, max_depth= 10)

rfc.fit(X_train, y_train)
rfc.score(X_test, y_test)


# *Prediting in X_test

# In[75]:

y_pred_loanstatus= rfc.predict(X_test)


# In[76]:

from sklearn import metrics
print metrics.accuracy_score(y_pred_loanstatus, y_test)


# In[77]:

report = metrics.classification_report(y_pred_loanstatus, y_test)
print report


# *Logistic Regression (1st method): accuracy score = 0.74; f1 score = 0.81

# *2nd method: GridSearchCV using make_classification; use n_samples = total rows from df (135696), #n_features as number of total columns (26)

# In[96]:

from sklearn.grid_search import GridSearchCV
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier

# Build a classification task using 3 informative features
X, y = make_classification(n_samples=135696,
                           #n_features=26,  
                           #n_informative=3,
                           #n_redundant=0,
                           #n_repeated=0,
                           #n_classes=2,
                           random_state=0,
                           shuffle=False)


rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=500, oob_score = True) 

param_grid = {'n_estimators': [100, 200, 300],
              'max_features': ['auto', 'sqrt', 'log2']}

grid = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 2)
grid.fit(X_train, y_train)
print grid.best_params_
print grid.best_score_
print grid.best_estimator_


# *Now, we can put "best" parameters into random forest classifier; Fitting in training, scoring in testing

# In[98]:

rfc = RandomForestClassifier(max_features= 'log2', n_estimators= 300)


# In[100]:

rfc.fit(X_train, y_train)
rfc.score(X_test, y_test)


# *Prediting in X_test

# In[101]:

y_pred_loanstatus = rfc.predict(X_test)


# In[102]:

from sklearn import metrics
print metrics.accuracy_score(y_pred_loanstatus, y_test)


# In[103]:

report = metrics.classification_report(y_pred_loanstatus, y_test)
print report


# *Logistic Regression (2nd method): accuracy score = 0.74; f1 score = 0.79

# 3.. Gradient boosting - 1st method

# In[90]:

y_train.shape


# *We only need to run GridSearchCV once, so we don't have to re-run here, but we can add "learning rate" parameter for Gradient boosting

# In[91]:

from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier()

param_grid = {"learning_rate": [0.1, 0.5]}

grid_search = GridSearchCV(estimator=gbc, param_grid=param_grid, n_jobs=-1, cv=2)
grid_search.fit(X_train, y_train)
print grid_search.best_params_
print grid_search.best_score_
print grid_search.best_estimator_


# *Now, we can put "best" parameters(from previous modeling part) & adding learning rate parameter into gradient boosting classifier; Fitting in training, scoring in testing

# In[92]:

gbc = GradientBoostingClassifier(max_features= 5, min_samples_split= 2, n_estimators= 100, max_depth= 10, learning_rate= 0.1)

gbc.fit(X_train, y_train)
gbc.score(X_test, y_test)


# *Prediting in X_test

# In[93]:

y_pred_loanstatus= gbc.predict(X_test)


# In[94]:

from sklearn import metrics
print metrics.accuracy_score(y_pred_loanstatus, y_test)


# In[95]:

report = metrics.classification_report(y_pred_loanstatus, y_test)
print report


# *Gradient Boosting(1st method): accuracy score = 0.74; f1 score = 0.78

# Gradient boosting - 2nd method:

# In[104]:

gbc = GradientBoostingClassifier(max_features= 'log2', n_estimators= 300, learning_rate= 0.1)


# In[105]:

gbc.fit(X_train, y_train)
gbc.score(X_test, y_test)


# In[106]:

y_pred_loanstatus= gbc.predict(X_test)


# In[107]:

from sklearn import metrics
print metrics.accuracy_score(y_pred_loanstatus, y_test)


# In[108]:

report = metrics.classification_report(y_pred_loanstatus, y_test)
print report


# *Gradient Boosting(2nd method): accuracy score = 0.74; f1 score = 0.79
